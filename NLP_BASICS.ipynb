{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["0KRe6eWt54Sn","H0zIeJuE8jq5","LDqzBaQsE9el","vptHwYGfF4ae","VvFjZHu6G4Dl","kVdFEECBIDhF"],"authorship_tag":"ABX9TyMtBxKCFIpq5bWZ1d58sUtB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Regular Expressions (RE)"],"metadata":{"id":"0KRe6eWt54Sn"}},{"cell_type":"markdown","source":["A Regular Expression or RegEx is a special sequence of characters that uses a search pattern to find a string or set of strings.\n","\n","It can detect the presence or absence of a text by matching it with a particular pattern and also can split a pattern into one or more sub-patterns."],"metadata":{"id":"uKNT4_bL5_ne"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9M-QahgL5qWk","executionInfo":{"status":"ok","timestamp":1747110787192,"user_tz":-360,"elapsed":81,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"e1a1e508-5189-41e1-ff8a-d9f4a1157c8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Start Index: 0\n","End Index: 5\n"]}],"source":["import re\n","\n","s = 'Hello from CSE475'\n","\n","match = re.search(r'Hello', s)\n","\n","print('Start Index:', match.start())\n","print('End Index:', match.end())"]},{"cell_type":"markdown","source":["Here r character (r’Hello’) stands for raw, not regex. The raw string is slightly different from a regular string, it won’t interpret the \\ character as an escape character. This is because the regular expression engine uses \\ character for its own escaping purpose."],"metadata":{"id":"SEl3z2l26Pe2"}},{"cell_type":"markdown","source":["##re.findall()"],"metadata":{"id":"vkHxrOO_6URm"}},{"cell_type":"markdown","source":["Return all non-overlapping matches of pattern in string, as a list of strings. The string is scanned left-to-right, and matches are returned in the order found."],"metadata":{"id":"9eD8ziFy6W3O"}},{"cell_type":"code","source":["import re\n","string = \"\"\"Hello my Number is 123456789 and\n","            my friend's number is 987654321\"\"\"\n","regex = '\\d+'\n","\n","match = re.findall(regex, string)\n","print(match)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zW3b5bmh6ZvI","executionInfo":{"status":"ok","timestamp":1747110787203,"user_tz":-360,"elapsed":21,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"0eff1f53-fb3d-4813-c110-c15b559fd572"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['123456789', '987654321']\n"]}]},{"cell_type":"markdown","source":["This code uses a regular expression (\\d+) to find all the sequences of one or more digits in the given string. It searches for numeric values and stores them in a list. In this example, it finds and prints the numbers “123456789” and “987654321” from the input string."],"metadata":{"id":"SKX-rHyH6fQ2"}},{"cell_type":"markdown","source":["##re.compile()"],"metadata":{"id":"05-upcZz6h1e"}},{"cell_type":"markdown","source":["Regular expressions are compiled into pattern objects, which have methods for various operations such as searching for pattern matches or performing string substitutions."],"metadata":{"id":"jhIBhOcW6lfv"}},{"cell_type":"code","source":["import re\n","p = re.compile('[a-e]')\n","\n","print(p.findall(\"Aye, said Mr. Gibenson Stark\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PQnlyrz86i--","executionInfo":{"status":"ok","timestamp":1747110787208,"user_tz":-360,"elapsed":18,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"fc43b4c6-3ac8-467a-93b7-f828ed111989"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['e', 'a', 'd', 'b', 'e', 'a']\n"]}]},{"cell_type":"markdown","source":["Understanding the Output:\n","\n","First occurrence is ‘e’ in “Aye” and not ‘A’, as it is Case Sensitive.\n","Next Occurrence is ‘a’ in “said”, then ‘d’ in “said”, followed by ‘b’ and ‘e’ in “Gibenson”, the Last ‘a’ matches with “Stark”.\n","Metacharacter backslash ‘\\’ has a very important role as it signals various sequences. If the backslash is to be used without its special meaning as metacharacter, use’\\\\’"],"metadata":{"id":"-2Zf2snJ60X2"}},{"cell_type":"code","source":["import re\n","p = re.compile('\\d')\n","print(p.findall(\"I went to him at 11 A.M. on 4th July 1886\"))\n","\n","p = re.compile('\\d+')\n","print(p.findall(\"I went to him at 11 A.M. on 4th July 1886\"))"],"metadata":{"id":"eVvxyvOM66tH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747114708940,"user_tz":-360,"elapsed":55,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"d0032676-8ae6-479a-db17-3cdb969bc99f"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["['1', '1', '4', '1', '8', '8', '6']\n","['11', '4', '1886']\n"]}]},{"cell_type":"markdown","source":["The code uses regular expressions to find and list all single digits and sequences of digits in the given input strings. It finds single digits with \\d and sequences of digits with \\d+."],"metadata":{"id":"Omr2SVUO67iv"}},{"cell_type":"markdown","source":["##re.split()"],"metadata":{"id":"XI2srZa06n7e"}},{"cell_type":"markdown","source":["Split string by the occurrences of a character or a pattern, upon finding that pattern, the remaining characters from the string are returned as part of the resulting list."],"metadata":{"id":"hP0MEBQG6_2n"}},{"cell_type":"code","source":["from re import split\n","\n","print(split('\\W+', 'Words, words , Words'))\n","print(split('\\W+', \"Word's words Words\"))\n","print(split('\\W+', 'On 12th Jan 2016, at 11:02 AM'))\n","print(split('\\d+', 'On 12th Jan 2016, at 11:02 AM'))"],"metadata":{"id":"j7zqDHCb620v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747114752853,"user_tz":-360,"elapsed":50,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"ffe7a5af-2d8c-4a8d-f277-6f4dd224d38f"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["['Words', 'words', 'Words']\n","['Word', 's', 'words', 'Words']\n","['On', '12th', 'Jan', '2016', 'at', '11', '02', 'AM']\n","['On ', 'th Jan ', ', at ', ':', ' AM']\n"]}]},{"cell_type":"markdown","source":["This code splits a string using non-word characters and spaces as delimiters, returning words: ['Words', 'words', 'Words']. Considers apostrophes as non-word characters: ['Word', 's', 'words', 'Words']. Splits using non-word characters and digits:['On', '12th', 'Jan', '2016', 'at', '11', '02', 'AM']. Splits using digits as the delimiter: ['On ', 'th Jan ', ', at ', ':', ' AM']."],"metadata":{"id":"-XjFli1-7JyP"}},{"cell_type":"markdown","source":["##re.search()"],"metadata":{"id":"t5vqEAf97z4w"}},{"cell_type":"markdown","source":["This method either returns None (if the pattern doesn’t match), or a re.MatchObject contains information about the matching part of the string. This method stops after the first match, so this is best suited for testing a regular expression more than extracting data."],"metadata":{"id":"sOut2xB_7yVw"}},{"cell_type":"code","source":["import re\n","regex = r\"([a-zA-Z]+) (\\d+)\"\n","\n","match = re.search(regex, \"I was born on June 24\")\n","if match != None:\n","    print (\"Match at index %s, %s\" % (match.start(), match.end()))\n","    print (\"Full match: %s\" % (match.group(0)))\n","    print (\"Month: %s\" % (match.group(1)))\n","    print (\"Day: %s\" % (match.group(2)))\n","\n","else:\n","    print (\"The regex pattern does not match.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pBEU3z4G8UrR","executionInfo":{"status":"ok","timestamp":1747111208949,"user_tz":-360,"elapsed":25,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"cbcef87d-220b-41db-a51d-1b663b66a41a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Match at index 14, 21\n","Full match: June 24\n","Month: June\n","Day: 24\n"]}]},{"cell_type":"markdown","source":["This Python code uses the re module to search for a date format (a word followed by a number, like \"June 24\") in the string \"I was born on June 24\". It defines a regular expression pattern r\"([a-zA-Z]+) (\\d+)\", which captures a word and a number. If a match is found, it prints the position of the match in the string, the full matched text, and the individual components (month and day). If no match is found, it notifies that the pattern does not match.\n"],"metadata":{"id":"fpkorsFGKrR2"}},{"cell_type":"markdown","source":["# Tokenization"],"metadata":{"id":"H0zIeJuE8jq5"}},{"cell_type":"markdown","source":["##Word Tokenization"],"metadata":{"id":"rvENMsgY8ooY"}},{"cell_type":"markdown","source":["With the help of nltk.tokenize.word_tokenize() method, we are able to extract the tokens from string of characters by using tokenize.word_tokenize() method. It actually returns the syllables from a single word. A single word can contain one or two syllables."],"metadata":{"id":"30s74WOE8_0Z"}},{"cell_type":"code","source":["from nltk import SyllableTokenizer\n","from nltk import word_tokenize\n","\n","# Create a reference variable for Class word_tokenize\n","tk = SyllableTokenizer()\n","\n","# Create a string input\n","gfg = \"Antidisestablishmentarianism\"\n","\n","# Use tokenize method\n","geek = tk.tokenize(gfg)\n","\n","print(geek)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HLqIhaX29A2M","executionInfo":{"status":"ok","timestamp":1747111598183,"user_tz":-360,"elapsed":17,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"fbfcc46c-674d-4ef9-96db-dd245af4e46f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Anti', 'di', 'ses', 'ta', 'blis', 'hmen', 'ta', 'ria', 'nism']\n"]}]},{"cell_type":"markdown","source":["##Rule-based Tokenization"],"metadata":{"id":"dzheIOXL96Xi"}},{"cell_type":"markdown","source":["Rule-based tokenization is a technique where a set of rules is applied to the input text to split it into tokens. These rules can be based on different criteria, such as whitespace, punctuation, regular expressions, or language-specific rules."],"metadata":{"id":"pUitmHcX-ELz"}},{"cell_type":"code","source":["# Step 1: Load the input text\n","text = \"The quick brown fox jumps over the lazy dog.\"\n","\n","# Step 2: Define the tokenization rules (split on whitespace)\n","tokens = text.split()\n","\n","# Step 4: Output the tokens\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1aisNXym9-mq","executionInfo":{"status":"ok","timestamp":1747111683478,"user_tz":-360,"elapsed":52,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"cbcd937d-8fa8-4e75-e5d9-6d5b3c7c0ded"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']\n"]}]},{"cell_type":"code","source":["import re\n","\n","#Load the input text\n","text = \"Hello, I am working at Geeks-for-Geeks and my email is abc23@gfg.com.\"\n","\n","#Define the regular expression pattern\n","p='([\\w]+-[\\w]+-[\\w]+)|([\\w\\.-]+@[\\w]+.[\\w]+)'\n","\n","# Find matches\n","matches = re.findall(p, text)\n","# print output\n","for match in matches:\n","    if match[0]:\n","        print(f\"Company Name: {match[0]}\")\n","    else:\n","        print(f\"Email address: {match[1]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JZvvXK5E-N0a","executionInfo":{"status":"ok","timestamp":1747115210087,"user_tz":-360,"elapsed":24,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"f9f91c9b-2bd6-4e99-b490-32489fdabde9"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Company Name: Geeks-for-Geeks\n","Email address: abc23@gfg.com\n"]}]},{"cell_type":"code","source":["import re\n","\n","# Load the input text\n","text = \"Hello! How can I help you?\"\n","\n","# Define the regular expression pattern\n","# Matches one or more non-alphanumeric characters\n","pattern = r'\\W+'\n","\n","# Remove the punctuation and get the resulting string\n","result = re.sub(pattern, ' ', text)\n","\n","# tokenize\n","tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', result)\n","\n","# Print the result\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yiNUCN0K-bIz","executionInfo":{"status":"ok","timestamp":1747111764876,"user_tz":-360,"elapsed":10,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"5516250b-0f28-409c-bc0d-ea3579ee10cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', 'How', 'can', 'I', 'help', 'you']\n"]}]},{"cell_type":"markdown","source":["##Subword Tokenization"],"metadata":{"id":"ByBJQvLb-u9T"}},{"cell_type":"markdown","source":["Subword Tokenization is a Natural Language Processing technique(NLP) in which a word is split into subwords and these subwords are known as tokens. This technique is used in any NLP task where a model needs to maintain a large vocabulary and complex word structures. The concept behind this, frequently occurring words should be in the vocabulary whereas rare words are split into frequent subwords. For example, the word \"unwanted\" might be split into \"un\", \"want\", and \"ed\". The word \"football\" might be split into \"foot\", and \"ball\"."],"metadata":{"id":"VrDwJULz-1c7"}},{"cell_type":"code","source":["import re\n","\n","test_str = \"\"\"\n","GeeksforGeeks is a fantastic resource for geeks\n","who are looking to enhance their programming skills,\n","and if you're a geek who wants to become an expert programmer,\n","then GeeksforGeeks is definitely the go-to place for geeks like you.\n","\"\"\"\n","# printing original String\n","print(\"The original string is : \" + str(test_str))\n","test_str=test_str.lower()\n","# using findall() to get all regex matches.\n","res = re.findall( r'\\w+|[^\\s\\w]+', test_str)\n","\n","# printing result\n","print(\"The converted string :\\n\" + str(res))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xO8ryX4b-xSj","executionInfo":{"status":"ok","timestamp":1747111956447,"user_tz":-360,"elapsed":46,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"652cc3e6-cfd9-4693-e61d-fdfeb73d3172"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The original string is : \n","GeeksforGeeks is a fantastic resource for geeks \n","who are looking to enhance their programming skills, \n","and if you're a geek who wants to become an expert programmer, \n","then GeeksforGeeks is definitely the go-to place for geeks like you.\n","\n","The converted string :\n","['geeksforgeeks', 'is', 'a', 'fantastic', 'resource', 'for', 'geeks', 'who', 'are', 'looking', 'to', 'enhance', 'their', 'programming', 'skills', ',', 'and', 'if', 'you', \"'\", 're', 'a', 'geek', 'who', 'wants', 'to', 'become', 'an', 'expert', 'programmer', ',', 'then', 'geeksforgeeks', 'is', 'definitely', 'the', 'go', '-', 'to', 'place', 'for', 'geeks', 'like', 'you', '.']\n"]}]},{"cell_type":"markdown","source":["Since we are taking each word. it creates a large dictionary and because of this, word tokenization can have an exploding vocabulary problem. To get rid of this problem we use tokenization on characters. Character tokens solve this large vocabulary problem. For that, we need to create a dictionary that has the frequency of each word in the sentence after the word tokenization and separate each word by space."],"metadata":{"id":"3gZNLsQm_Ffk"}},{"cell_type":"code","source":["from collections import OrderedDict\n","res_dict=OrderedDict()\n","for i in res:\n","    new_string=' '.join(char for char in i)\n","    if new_string in res_dict:\n","        res_dict[new_string]+=1\n","    else:\n","        res_dict[new_string]=1\n","res_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O0xWXj6X_J1I","executionInfo":{"status":"ok","timestamp":1747111960645,"user_tz":-360,"elapsed":5,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"3a317e4e-f415-4f30-d6ca-9b2c79062dff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('g e e k s f o r g e e k s', 2),\n","             ('i s', 2),\n","             ('a', 2),\n","             ('f a n t a s t i c', 1),\n","             ('r e s o u r c e', 1),\n","             ('f o r', 2),\n","             ('g e e k s', 2),\n","             ('w h o', 2),\n","             ('a r e', 1),\n","             ('l o o k i n g', 1),\n","             ('t o', 3),\n","             ('e n h a n c e', 1),\n","             ('t h e i r', 1),\n","             ('p r o g r a m m i n g', 1),\n","             ('s k i l l s', 1),\n","             (',', 2),\n","             ('a n d', 1),\n","             ('i f', 1),\n","             ('y o u', 2),\n","             (\"'\", 1),\n","             ('r e', 1),\n","             ('g e e k', 1),\n","             ('w a n t s', 1),\n","             ('b e c o m e', 1),\n","             ('a n', 1),\n","             ('e x p e r t', 1),\n","             ('p r o g r a m m e r', 1),\n","             ('t h e n', 1),\n","             ('d e f i n i t e l y', 1),\n","             ('t h e', 1),\n","             ('g o', 1),\n","             ('-', 1),\n","             ('p l a c e', 1),\n","             ('l i k e', 1),\n","             ('.', 1)])"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["#Lemmatization"],"metadata":{"id":"LDqzBaQsE9el"}},{"cell_type":"markdown","source":["Lemmatization techniques in natural language processing (NLP) involve methods to identify and transform words into their base or root forms, known as lemmas. These approaches contribute to text normalization, facilitating more accurate language analysis and processing in various NLP applications."],"metadata":{"id":"XbIpEI5aE7gF"}},{"cell_type":"code","source":["# import these modules\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","\n","try:\n","    nltk.data.find('corpora/wordnet')\n","except LookupError:\n","    nltk.download('wordnet')"],"metadata":{"id":"GJKxDIg5Fuum"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lemmatizer = WordNetLemmatizer()\n","\n","print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n","print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n","\n","# a denotes adjective in \"pos\"\n","print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSoX6nN8FLQl","executionInfo":{"status":"ok","timestamp":1747113654904,"user_tz":-360,"elapsed":5329,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"ee2813c7-a259-41f5-c554-ce840e7ab194"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["rocks : rock\n","corpora : corpus\n","better : good\n"]}]},{"cell_type":"code","source":["import spacy\n","\n","# Load the spaCy English model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Define a sample text\n","text = \"The quick brown foxes are jumping over the lazy dogs.\"\n","\n","# Process the text using spaCy\n","doc = nlp(text)\n","\n","# Extract lemmatized tokens\n","lemmatized_tokens = [token.lemma_ for token in doc]\n","\n","# Join the lemmatized tokens into a sentence\n","lemmatized_text = ' '.join(lemmatized_tokens)\n","\n","# Print the original and lemmatized text\n","print(\"Original Text:\", text)\n","print(\"Lemmatized Text:\", lemmatized_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HtHiTxI2FOE9","executionInfo":{"status":"ok","timestamp":1747113572796,"user_tz":-360,"elapsed":12213,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"0670c6fe-fcf7-4939-a939-7fa4817e427e"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Text: The quick brown foxes are jumping over the lazy dogs.\n","Lemmatized Text: the quick brown fox be jump over the lazy dog .\n"]}]},{"cell_type":"markdown","source":["#Stemming"],"metadata":{"id":"vptHwYGfF4ae"}},{"cell_type":"markdown","source":["Stemming is a method in text processing that eliminates prefixes and suffixes from words, transforming them into their fundamental or root form, The main objective of stemming is to streamline and standardize words, enhancing the effectiveness of the natural language processing tasks."],"metadata":{"id":"7i2h7TBAGMt-"}},{"cell_type":"markdown","source":["##Porter’s Stemmer"],"metadata":{"id":"oYNlSPrRGTg1"}},{"cell_type":"markdown","source":[" It is one of the most popular stemming methods proposed in 1980. It is based on the idea that the suffixes in the English language are made up of a combination of smaller and simpler suffixes. This stemmer is known for its speed and simplicity. The main applications of Porter Stemmer include data mining and Information retrieval. However, its applications are only limited to English words. Also, the group of stems is mapped on to the same stem and the output stem is not necessarily a meaningful word. The algorithms are fairly lengthy in nature and are known to be the oldest stemmer."],"metadata":{"id":"iN005SW-GtyN"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","\n","# Create a Porter Stemmer instance\n","porter_stemmer = PorterStemmer()\n","\n","# Example words for stemming\n","words = [\"running\", \"jumps\", \"happily\", \"running\", \"happily\"]\n","\n","# Apply stemming to each word\n","stemmed_words = [porter_stemmer.stem(word) for word in words]\n","\n","# Print the results\n","print(\"Original words:\", words)\n","print(\"Stemmed words:\", stemmed_words)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"79S333mNGNed","executionInfo":{"status":"ok","timestamp":1747113831035,"user_tz":-360,"elapsed":14,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"2b23abed-e879-44ae-fa03-43f81dc1bb18"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Original words: ['running', 'jumps', 'happily', 'running', 'happily']\n","Stemmed words: ['run', 'jump', 'happili', 'run', 'happili']\n"]}]},{"cell_type":"markdown","source":["##Snowball Stemmer"],"metadata":{"id":"VQ071JgzGXht"}},{"cell_type":"markdown","source":["The Snowball Stemmer, compared to the Porter Stemmer, is multi-lingual as it can handle non-English words. It supports various languages and is based on the ‘Snowball’ programming language, known for efficient processing of small strings.\n","\n","The Snowball stemmer is way more aggressive than Porter Stemmer and is also referred to as Porter2 Stemmer. Because of the improvements added when compared to the Porter Stemmer, the Snowball stemmer is having greater computational speed."],"metadata":{"id":"QayfCw60GsH2"}},{"cell_type":"code","source":["from nltk.stem import SnowballStemmer\n","\n","# Choose a language for stemming, for example, English\n","stemmer = SnowballStemmer(language='english')\n","\n","# Example words to stem\n","words_to_stem = ['running', 'jumped', 'happily', 'quickly', 'foxes']\n","\n","# Apply Snowball Stemmer\n","stemmed_words = [stemmer.stem(word) for word in words_to_stem]\n","\n","# Print the results\n","print(\"Original words:\", words_to_stem)\n","print(\"Stemmed words:\", stemmed_words)\n"],"metadata":{"id":"ptHpjoC7GcVt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Lancaster Stemmer"],"metadata":{"id":"qqyiqk5lGg9F"}},{"cell_type":"markdown","source":["The Lancaster stemmers are more aggressive and dynamic compared to the other two stemmers. The stemmer is really faster, but the algorithm is really confusing when dealing with small words. But they are not as efficient as Snowball Stemmers. The Lancaster stemmers save the rules externally and basically uses an iterative algorithm."],"metadata":{"id":"-oZpIS3YGiZN"}},{"cell_type":"code","source":["from nltk.stem import LancasterStemmer\n","\n","# Create a Lancaster Stemmer instance\n","stemmer = LancasterStemmer()\n","\n","# Example words to stem\n","words_to_stem = ['running', 'jumped', 'happily', 'quickly', 'foxes']\n","\n","# Apply Lancaster Stemmer\n","stemmed_words = [stemmer.stem(word) for word in words_to_stem]\n","\n","# Print the results\n","print(\"Original words:\", words_to_stem)\n","print(\"Stemmed words:\", stemmed_words)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QGYunJxTGljd","executionInfo":{"status":"ok","timestamp":1747113901703,"user_tz":-360,"elapsed":6,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"328a7ad2-65f0-4a20-9599-afd21f355112"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Original words: ['running', 'jumped', 'happily', 'quickly', 'foxes']\n","Stemmed words: ['run', 'jump', 'happy', 'quick', 'fox']\n"]}]},{"cell_type":"markdown","source":["#Removing stop words"],"metadata":{"id":"VvFjZHu6G4Dl"}},{"cell_type":"markdown","source":["##Removing stop words with NLTK"],"metadata":{"id":"eBM3qMafHADN"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import nltk\n","\n","# Download the 'punkt_tab' resource if not found\n","try:\n","    nltk.data.find('tokenizers/punkt_tab')\n","except LookupError:\n","    nltk.download('punkt_tab')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xpttrvr4HO2N","executionInfo":{"status":"ok","timestamp":1747114094089,"user_tz":-360,"elapsed":260,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"646b1ecc-96bb-4c11-9780-cbe8f28f0a1a"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]}]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","example_sent = \"\"\"This is a sample sentence,\n","\t\t\t\tshowing off the stop words filtration.\"\"\"\n","\n","stop_words = set(stopwords.words('english'))\n","\n","word_tokens = word_tokenize(example_sent)\n","# converts the words in word_tokens to lower case and then checks whether\n","#they are present in stop_words or not\n","filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n","#with no lower case conversion\n","filtered_sentence = []\n","\n","for w in word_tokens:\n","\tif w not in stop_words:\n","\t\tfiltered_sentence.append(w)\n","\n","print(word_tokens)\n","print(filtered_sentence)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GPaI0mLiHFc9","executionInfo":{"status":"ok","timestamp":1747114097388,"user_tz":-360,"elapsed":36,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"25e50d7b-6dd5-41f1-c1ba-a379be7e5537"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n","['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"]}]},{"cell_type":"markdown","source":["In the first step, the sample sentence, which reads “This is a sample sentence, showing off the stop words filtration,” is tokenized into words using the word_tokenize function. The code then filters out stopwords by converting each word to lowercase and checking its presence in the set of English stopwords obtained from NLTK. The resulting filtered_sentence is printed, showcasing both lowercased and original versions, providing a cleaned version of the sentence with common English stopwords removed."],"metadata":{"id":"krx4OtDfHfi9"}},{"cell_type":"markdown","source":["##Removing stop words with SpaCy"],"metadata":{"id":"iwZb7fZEHmtO"}},{"cell_type":"code","source":["import spacy\n","\n","# Load spaCy English model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Sample text\n","text = \"There is a pen on the table\"\n","\n","# Process the text using spaCy\n","doc = nlp(text)\n","\n","# Remove stopwords\n","filtered_words = [token.text for token in doc if not token.is_stop]\n","\n","# Join the filtered words to form a clean text\n","clean_text = ' '.join(filtered_words)\n","\n","print(\"Original Text:\", text)\n","print(\"Text after Stopword Removal:\", clean_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RkipeEvXHp9O","executionInfo":{"status":"ok","timestamp":1747114207978,"user_tz":-360,"elapsed":846,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"290e2bd8-fbd1-4029-9dc6-d64408c6b9a6"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Text: There is a pen on the table\n","Text after Stopword Removal: pen table\n"]}]},{"cell_type":"markdown","source":["The provided Python code utilizes the spaCy library for natural language processing to remove stopwords from a sample text. Initially, the spaCy English model is loaded, and the sample text, “There is a pen on the table,” is processed using spaCy. Stopwords are then filtered out from the processed tokens, and the resulting non-stopword tokens are joined to create a clean version of the text."],"metadata":{"id":"amVsc8J0HtPa"}},{"cell_type":"markdown","source":["##Removing stop words with SkLearn"],"metadata":{"id":"mkIL1gYSH5oN"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","# Another sample text\n","new_text = \"The quick brown fox jumps over the lazy dog.\"\n","\n","# Tokenize the new text using NLTK\n","new_words = word_tokenize(new_text)\n","\n","# Remove stopwords using NLTK\n","new_filtered_words = [\n","\tword for word in new_words if word.lower() not in stopwords.words('english')]\n","\n","# Join the filtered words to form a clean text\n","new_clean_text = ' '.join(new_filtered_words)\n","\n","print(\"Original Text:\", new_text)\n","print(\"Text after Stopword Removal:\", new_clean_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XcfvW4iPH46N","executionInfo":{"status":"ok","timestamp":1747114248687,"user_tz":-360,"elapsed":10,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"54c53d6d-c89d-4529-a1a6-bf89613b2258"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Text: The quick brown fox jumps over the lazy dog.\n","Text after Stopword Removal: quick brown fox jumps lazy dog .\n"]}]},{"cell_type":"markdown","source":["#POS(Parts-Of-Speech) Tagging"],"metadata":{"id":"kVdFEECBIDhF"}},{"cell_type":"markdown","source":["##NLTK"],"metadata":{"id":"-xN5Hjh6IQv1"}},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U7NbQXZWIFJu","executionInfo":{"status":"ok","timestamp":1747114303037,"user_tz":-360,"elapsed":747,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"b50f1d01-45dc-4cd2-d436-541768b4ce4e"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["# Importing the NLTK library\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","\n","# Sample text\n","text = \"NLTK is a powerful library for natural language processing.\"\n","\n","# Performing PoS tagging\n","pos_tags = pos_tag(words)\n","\n","# Displaying the PoS tagged result in separate lines\n","print(\"Original Text:\")\n","print(text)\n","\n","print(\"\\nPoS Tagging Result:\")\n","for word, pos_tag in pos_tags:\n","\tprint(f\"{word}: {pos_tag}\")\n"],"metadata":{"id":"VaKBp34DIKd-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Import the NLTK library and its modules for tokenization. Tokenize the input text into words using word_tokenize. Use the pos_tag function from NLTK to perform part-of-speech tagging on the tokenized words. Print the original text and the resulting POS tags in separate lines, showing each word along with its corresponding part-of-speech tag."],"metadata":{"id":"AQoB3kCTIPJd"}},{"cell_type":"markdown","source":["##Spacy"],"metadata":{"id":"vGxrmE9mIVom"}},{"cell_type":"code","source":["#importing libraries\n","import spacy\n","\n","# Load the English language model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Sample text\n","text = \"SpaCy is a popular natural language processing library.\"\n","\n","# Process the text with SpaCy\n","doc = nlp(text)\n","\n","# Display the PoS tagged result\n","print(\"Original Text: \", text)\n","print(\"PoS Tagging Result:\")\n","for token in doc:\n","\tprint(f\"{token.text}: {token.pos_}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J3Lb5QmOIZLm","executionInfo":{"status":"ok","timestamp":1747114385872,"user_tz":-360,"elapsed":825,"user":{"displayName":"Rakib Hasan","userId":"17871142173441604144"}},"outputId":"176e4de9-9584-4b8b-9c3f-2e1d69791f38"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Text:  SpaCy is a popular natural language processing library.\n","PoS Tagging Result:\n","SpaCy: PROPN\n","is: AUX\n","a: DET\n","popular: ADJ\n","natural: ADJ\n","language: NOUN\n","processing: NOUN\n","library: NOUN\n",".: PUNCT\n"]}]}]}